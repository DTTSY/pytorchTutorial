{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BPaXFVYuZJ6S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything in pytorch is based on Tensor operations.\n",
        "A tensor can have different dimensions\n",
        "so it can be 1d, 2d, or even 3d and higher\n",
        "\n",
        "scalar, vector, matrix, tensor"
      ],
      "metadata": {
        "id": "sr68EcHpacqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.empty(size): uninitiallized\n",
        "x = torch.empty(1) # scalar\n",
        "print(x)\n",
        "x = torch.empty(3) # vector, 1D\n",
        "print(x)\n",
        "x = torch.empty(2,3) # matrix, 2D\n",
        "print(x)\n",
        "x = torch.empty(2,2,3) # tensor, 3 dimensions\n",
        "#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
        "print(x)"
      ],
      "metadata": {
        "id": "1wUhJp-Hax3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.rand(size): random numbers [0, 1]\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "JVxspfBDdojW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.zeros(size), fill with 0\n",
        "# torch.ones(size), fill with 1\n",
        "x = torch.zeros(5, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "J732AXAMdzVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check size\n",
        "print(x.size())\n",
        "\n",
        "# check data type\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "id": "k9r7sTByd5kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify types, float32 default\n",
        "x = torch.zeros(5, 3, dtype=torch.float16)\n",
        "print(x)\n",
        "\n",
        "# check type\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "id": "QTZUSqbKeH98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct from data\n",
        "x = torch.tensor([5.5, 3])\n",
        "print(x, x.size(),x.dtype)"
      ],
      "metadata": {
        "id": "93prQAbLeLYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad argument\n",
        "# This will tell pytorch that it will need to calculate the gradients for this tensor\n",
        "# later in your optimization steps\n",
        "# i.e. this is a variable in your model that you want to optimize\n",
        "x = torch.tensor([5.5, 3], requires_grad=True)"
      ],
      "metadata": {
        "id": "gZI8cb84erJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Operations\n",
        "y = torch.rand(2, 2)\n",
        "x = torch.rand(2, 2)\n",
        "\n",
        "# elementwise addition\n",
        "z = x + y\n",
        "# torch.add(x,y)\n",
        "\n",
        "# in place addition, everythin with a trailing underscore is an inplace operation\n",
        "# i.e. it will modify the variable\n",
        "# y.add_(x)\n",
        "\n",
        "# substraction\n",
        "z = x - y\n",
        "z = torch.sub(x, y)\n",
        "\n",
        "# multiplication\n",
        "z = x * y\n",
        "z = torch.mul(x,y)\n",
        "\n",
        "# division\n",
        "z = x / y\n",
        "z = torch.div(x,y)"
      ],
      "metadata": {
        "id": "BXk9lisXeuPE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing\n",
        "x = torch.rand(5,3)\n",
        "print(x)\n",
        "print(x[:, 0]) # all rows, column 0\n",
        "print(x[1, :]) # row 1, all columns\n",
        "print(x[1,1]) # element at 1, 1"
      ],
      "metadata": {
        "id": "Yly_HNQIe3T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the actual value if only 1 element in your tensor\n",
        "print(x[1,1])\n",
        "print(x[1,1].item())"
      ],
      "metadata": {
        "id": "rSD2NVnJfTSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape with torch.view()\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "# if -1 it pytorch will automatically determine the necessary size\n",
        "print(x.size(), y.size(), z.size())"
      ],
      "metadata": {
        "id": "LHiSR6i5fkcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy\n",
        "# Converting a Torch Tensor to a NumPy array and vice versa is very easy\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "\n",
        "# torch to numpy with .numpy()\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "print(type(b))"
      ],
      "metadata": {
        "id": "kbAPO0F8fuGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carful: If the Tensor is on the CPU (not the GPU),\n",
        "# both objects will share the same memory location, so changing one\n",
        "# will also change the other\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xJK01x0f8KC",
        "outputId": "d25ccd90-48cc-40ff-ee1a-496933cca345"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy to torch with .from_numpy(x)\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a)\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "Z_jiuHOsgLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again be careful when modifying\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "AtlWIkGPgb6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# by default all tensors are created on the CPU,\n",
        "# but you can also move them to the GPU (only if it's available )\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # a CUDA device object\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
        "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "    z = x + y\n",
        "    # z = z.numpy() # not possible because numpy cannot handle GPU tenors\n",
        "    # move to CPU again\n",
        "    z.to(\"cpu\")       # ``.to`` can also change dtype together!\n",
        "    # z = z.numpy()"
      ],
      "metadata": {
        "id": "CKQtet3uglcG"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}